{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import nltk\n",
    "import spacy\n",
    "import math\n",
    "import random\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('parsed_text.txt').read()\n",
    "SOS = '<SOS>'\n",
    "EOS = '<EOS>'\n",
    "UNK = '<UNK>'\n",
    "n = 2                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_special_tokens(sentences, n):\n",
    "    return [f\"{(SOS + ' ') * numpy.clip(n-1, 1, 1000)}{sentence} {EOS}\" for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unknown_tokens(tokens):\n",
    "    freqs = nltk.FreqDist(tokens)\n",
    "    return [token if freqs[token] > 1 else UNK for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, n):\n",
    "    doc = nlp(text)\n",
    "    sentences = [sentence.text.lower() for sentence in doc.sents]\n",
    "    sentences = add_special_tokens(sentences, n)\n",
    "    tokens = ' '.join(sentences).split(' ')\n",
    "    tokens = create_unknown_tokens(tokens)\n",
    "    return sentences, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, tokens = preprocess_text(text, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3049"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = nltk.FreqDist(tokens)\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grams = nltk.ngrams(tokens, n)\n",
    "n_vocab = nltk.FreqDist(n_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_grams = nltk.ngrams(tokens, n-1)\n",
    "m_vocab = nltk.FreqDist(m_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothed_count(n_gram, n_count, k, m_vocab, vocab_size):\n",
    "    m_gram = n_gram[:-1]\n",
    "    m_count = m_vocab[m_gram]\n",
    "    return -math.log((n_count + k) / (m_count + k * vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = {n_gram: smoothed_count(n_gram, count, 1, m_vocab, vocab_size) for n_gram, count in n_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "406"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_next_word(probabilities, m_gram, blacklist):\n",
    "    candidates = {}\n",
    "    blacklist = blacklist + [UNK]\n",
    "\n",
    "    for n_gram, probability in probabilities.items():\n",
    "        if n_gram[:-1] == m_gram:                                                                  # find n_grams based on m_gram\n",
    "            candidate = n_gram[-1]\n",
    "            if candidate not in blacklist:\n",
    "                candidates[candidate] = probability                                                # check that new word is not in blacklist\n",
    "\n",
    "    candidates = sorted(candidates.items(), key=lambda probability: probability[1], reverse=True)  # sort by probability\n",
    "\n",
    "    if len(candidates) == 0:\n",
    "        return (EOS, 1)                                                                            # if no other options EOS\n",
    "    else:\n",
    "        return random.choice(candidates[:min(len(candidates), 15)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentences(probabilities, n, n_sent, max_len = 25):\n",
    "    for _ in range(n_sent):\n",
    "        sentence = [SOS] * numpy.clip(n-1, 1, 1000)                                                 # create new sentenses with SOS\n",
    "        prob = 1\n",
    "\n",
    "        while sentence[-1] != EOS:\n",
    "            prev_words = () if n == 1 else tuple(sentence[-(n-1):])                                 # find m_gram\n",
    "            blacklist = sentence + [EOS]                                                            # create blacklist so all words are unique\n",
    "            \n",
    "            next_word, next_prob = choose_next_word(probabilities, prev_words, blacklist)\n",
    "            sentence.append(next_word)\n",
    "            prob += next_prob\n",
    "\n",
    "            if len(sentence) >= max_len:\n",
    "                sentence.append(EOS)\n",
    "\n",
    "        print(' '.join(sentence), math.exp(-prob))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> data may <EOS> 2.905125753710693e-05\n",
      "<SOS> work under the book on learning for artificial neurons used by theoretical and study in cognitive systems had come to pattern recognition continued outside <EOS> 6.961145935354397e-44\n",
      "<SOS> other researchers were mostly with algorithms studied human cognitive systems had been <EOS> 1.1783461654611023e-22\n",
      "<SOS> from data and computer program is to pattern recognition continued outside the mathematical models of human cognitive systems had come <EOS> 1.1867802031612743e-36\n",
      "<SOS> by mathematical models which have been <EOS> 6.62916277420036e-12\n"
     ]
    }
   ],
   "source": [
    "generate_sentences(probabilities, n, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
