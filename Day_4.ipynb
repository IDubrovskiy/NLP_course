{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language prediction RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 25\n",
    "num_pairs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sent):\n",
    "    sent = re.sub(r\"([,.!?Â«Â»])\", r\"\", sent)\n",
    "    sent = re.sub(r\"(['])\", r\" \", sent)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pairs(path, max_len):\n",
    "\n",
    "    text = open(path, encoding='utf-8').read()\n",
    "    text = text.strip().split('\\n')\n",
    "    pairs = []\n",
    "\n",
    "    for pair in text:\n",
    "        sent = pair.split('\\t')\n",
    "        preprocessed_first_sent = preprocess(sent[0].lower()).strip()\n",
    "        preprocessed_second_sent = preprocess(sent[1].lower()).strip()\n",
    "        if (len(preprocessed_first_sent.split()) < max_len and len(preprocessed_second_sent.split()) < max_len):\n",
    "            pairs.append((preprocessed_first_sent, preprocessed_second_sent))\n",
    "          \n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13007\n",
      "76212\n"
     ]
    }
   ],
   "source": [
    "pairs_indo = read_pairs('Data/ind.txt', max_len)\n",
    "print(len(pairs_indo))\n",
    "\n",
    "pairs_dutch = read_pairs('Data/nld.txt', max_len)\n",
    "print(len(pairs_dutch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "\n",
    "for sent_eng, sent_indo in pairs_indo:\n",
    "    for word in sent_eng.split(\" \"):\n",
    "        vocab.add(word)\n",
    "    for word in sent_indo.split(\" \"):\n",
    "        vocab.add(word)\n",
    "for sent_eng, sent_dutch in pairs_dutch:\n",
    "    for word in sent_dutch.split(\" \"):\n",
    "        vocab.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24959"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(vocab):   \n",
    "    word_id, id_word = {}, {}\n",
    "    \n",
    "    word_id['<EOS>'] = 0\n",
    "    id_word[0] = '<EOS>'   \n",
    "\n",
    "    for i, word in enumerate(vocab):\n",
    "        word_id[word] = i + 1\n",
    "        id_word[i + 1] = word\n",
    "\n",
    "    return word_id, id_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_id, id_word = bag_of_words(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eng, data_indo, data_dutch = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent_eng, sent_indo in pairs_indo[0:num_pairs]:\n",
    "    data_eng.append(([word_id[word] for word in sent_eng.split(\" \")], 0))\n",
    "    data_indo.append(([word_id[word] for word in sent_indo.split(\" \")], 1))\n",
    "\n",
    "for _, sent_dutch in pairs_dutch[0:num_pairs]:\n",
    "    data_dutch.append(([word_id[word] for word in sent_dutch.split(\" \")], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "lr = 0.001\n",
    "epochs = 10\n",
    "bs = 64\n",
    "num_classes = 3\n",
    "\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, emb_size, hidden_size, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.emb_size = emb_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(num_embeddings=input_size, embedding_dim=emb_size) \n",
    "        self.gru = nn.GRU(emb_size, hidden_size, batch_first=True) \n",
    "        self.output = nn.Linear(hidden_size, num_classes) \n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)\n",
    "        _, hid = self.gru(emb)\n",
    "        out = self.output(hid)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(len(vocab)+1, 100, 50, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (embedding): Embedding(24960, 100)\n",
      "  (gru): GRU(100, 50, batch_first=True)\n",
      "  (output): Linear(in_features=50, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = data_eng + data_indo + data_dutch\n",
    "data_len = len(data_all)\n",
    "input_ids = np.zeros((data_len, max_len), dtype=np.int32)                           # make uniform inputs\n",
    "target_ids = np.zeros((data_len, num_classes), dtype=np.float64)                    # make one hot\n",
    "\n",
    "for index, (sent, label) in enumerate(data_all):\n",
    "    input_ids[index, :len(sent)] = sent\n",
    "    target_ids[index, label] = 1\n",
    "\n",
    "train_data = TensorDataset(torch.LongTensor(input_ids).to(device), torch.FloatTensor(target_ids).to(device))\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47/47 [00:00<00:00, 119.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss 87.10934091121592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47/47 [00:00<00:00, 405.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss 80.28364952574385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47/47 [00:00<00:00, 376.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2, loss 74.53903344336976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47/47 [00:00<00:00, 382.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3, loss 71.61094340872258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47/47 [00:00<00:00, 415.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4, loss 70.2987706610497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47/47 [00:00<00:00, 405.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5, loss 68.82320412169112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47/47 [00:00<00:00, 415.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6, loss 68.24786782771983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47/47 [00:00<00:00, 394.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7, loss 67.70297168163543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47/47 [00:00<00:00, 353.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8, loss 67.3416013514742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47/47 [00:00<00:00, 379.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9, loss 67.14562468833111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=lr)\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for x, y in tqdm(train_dataloader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = rnn(x)\n",
    "        \n",
    "        current_loss = loss_func(out, y.reshape(out.size()))\n",
    "        current_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += current_loss.item()\n",
    "\n",
    "    losses.append(current_loss.item())\n",
    "    print('epoch: {}, loss {}'.format(epoch, total_loss/len(train_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(rnn.state_dict(), \"rnn.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(24960, 100)\n",
       "  (gru): GRU(100, 50, batch_first=True)\n",
       "  (output): Linear(in_features=50, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = RNN(len(vocab)+1, 100, 50, 3)\n",
    "rnn = rnn.to(device)\n",
    "rnn.load_state_dict(torch.load(\"rnn.ckpt\"))\n",
    "rnn.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it s weird\n",
      "Indonesian\n",
      "\n",
      "ik heb gelijk\n",
      "Dutch\n",
      "\n",
      "i feel numb\n",
      "Indonesian\n",
      "\n",
      "vouw het\n",
      "Dutch\n",
      "\n",
      "dank u\n",
      "Dutch\n",
      "\n",
      "ontspan u\n",
      "Dutch\n",
      "\n",
      "lihat ini\n",
      "English\n",
      "\n",
      "are you sleepy\n",
      "English\n",
      "\n",
      "cium saya\n",
      "Dutch\n",
      "\n",
      "sekarang pukul 830\n",
      "Indonesian\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent, label in data_all[:10]:\n",
    "    print(\" \".join([id_word[word] for word in sent]))\n",
    "\n",
    "    x = torch.tensor(sent, dtype=torch.long, device=device)\n",
    "    y_pred = rnn(x)\n",
    "\n",
    "    if y_pred.argmax().item() == 0:\n",
    "        print('English\\n')\n",
    "\n",
    "    if y_pred.argmax().item() == 1:\n",
    "        print('Indonesian\\n')\n",
    "\n",
    "    if y_pred.argmax().item() == 2:\n",
    "        print('Dutch\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock sentiment prediction RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"zeroshot/twitter-financial-news-sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 9543\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_stock_dataset(data):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for text in data:\n",
    "        txt = text['text']\n",
    "        txt = re.sub(r'https\\S+', '', txt)                      # delete links\n",
    "        txt = re.sub(r'(\\$[A-Z]+)+', '', txt)                   # delete tickers\n",
    "        txt = re.sub(r'\\(.*\\)\\s', '', txt)                      # delete unnecessary symbols\n",
    "        txt = re.sub(r\"([-:,.!?Â«Â»])\", r\"\", txt)                 # delete unnecessary symbols\n",
    "        txt = txt.strip()\n",
    "        if (len(txt.split(\" \")) < max_len):\n",
    "            texts.append(txt)\n",
    "            labels.append(text['label'])\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = preprocess_stock_dataset(dataset['train'])\n",
    "x_test, y_test = preprocess_stock_dataset(dataset['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9526"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "\n",
    "for sent in x_train:\n",
    "    for word in sent.split(\" \"):\n",
    "        vocab.add(word)\n",
    "for sent in x_test:\n",
    "    for word in sent.split(\" \"):\n",
    "        vocab.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25081"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_id, id_word = bag_of_words(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = [], []\n",
    "for x, y in zip(x_train, y_train):\n",
    "    train_data.append(([word_id[word] for word in x.split(\" \")], y))\n",
    "\n",
    "for x, y in zip(x_test, y_test):\n",
    "    test_data.append(([word_id[word] for word in x.split(\" \")], y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "lr = 0.001\n",
    "epochs = 15\n",
    "bs = 64\n",
    "num_classes = 3\n",
    "\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, emb_size, hidden_size, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.emb_size = emb_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(num_embeddings=input_size, embedding_dim=emb_size) \n",
    "        self.gru = nn.GRU(emb_size, hidden_size, batch_first=True) \n",
    "        self.output = nn.Linear(hidden_size, num_classes) \n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)\n",
    "        _, hidden_state = self.gru(emb)\n",
    "        out = self.output(hidden_state)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(len(vocab)+1, 1000, 500, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (embedding): Embedding(25082, 1000)\n",
      "  (gru): GRU(1000, 500, batch_first=True)\n",
      "  (output): Linear(in_features=500, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = np.zeros((len(train_data), max_len), dtype=np.int32)\n",
    "target_ids = np.zeros((len(train_data), num_classes), dtype=np.float64)\n",
    "\n",
    "for index, (sent, label) in enumerate(train_data):\n",
    "    input_ids[index, :len(sent)] = sent\n",
    "    target_ids[index, label] = 1                                                                                    # creating one hot\n",
    "\n",
    "data_train = TensorDataset(torch.LongTensor(input_ids).to(device), torch.FloatTensor(target_ids).to(device))\n",
    "train_sampler = RandomSampler(data_train)\n",
    "train_dataloader = DataLoader(data_train, sampler=train_sampler, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11680, 14418, 20523, ...,     0,     0,     0],\n",
       "       [ 4511,  7038,  7368, ...,     0,     0,     0],\n",
       "       [14271, 12662,  5691, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [  952, 16918,  3962, ...,     0,     0,     0],\n",
       "       [18464, 13750,  1670, ...,     0,     0,     0],\n",
       "       [10303, 18104, 20705, ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 149/149 [00:01<00:00, 108.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss 84.67715244165203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 149/149 [00:01<00:00, 148.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss 77.39226744478981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 149/149 [00:00<00:00, 149.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2, loss 72.69254646045249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 149/149 [00:00<00:00, 149.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3, loss 71.08163787534573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 149/149 [00:00<00:00, 149.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4, loss 70.50410041553062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 149/149 [00:01<00:00, 148.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5, loss 70.43340629219209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 149/149 [00:01<00:00, 148.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6, loss 70.29190680484643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 149/149 [00:01<00:00, 143.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7, loss 70.165168915819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 149/149 [00:01<00:00, 148.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8, loss 70.12904007162824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 149/149 [00:00<00:00, 150.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9, loss 70.31301262874732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 149/149 [00:00<00:00, 152.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, loss 70.47438978668828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 149/149 [00:00<00:00, 150.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11, loss 70.23481141160798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 149/149 [00:00<00:00, 153.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12, loss 70.23566569897953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 149/149 [00:00<00:00, 152.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13, loss 70.21507273424392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 149/149 [00:00<00:00, 152.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14, loss 70.18170757421711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=lr)\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for x, y in tqdm(train_dataloader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = rnn(x)\n",
    "        \n",
    "        current_loss = loss_func(out, y.reshape(out.size()))\n",
    "        current_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += current_loss.item()\n",
    "\n",
    "    losses.append(current_loss.item())\n",
    "    print('epoch: {}, loss {}'.format(epoch, total_loss/len(train_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(rnn.state_dict(), \"stock_rnn.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(25082, 1000)\n",
       "  (gru): GRU(1000, 500, batch_first=True)\n",
       "  (output): Linear(in_features=500, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = RNN(len(vocab)+1, 1000, 500, 3)\n",
    "rnn = rnn.to(device)\n",
    "rnn.load_state_dict(torch.load(\"stock_rnn.ckpt\"))\n",
    "rnn.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CACI gains a bull on growth acceleration\n",
      "Predicted: Bullish\n",
      "\n",
      "Real: Bullish\n",
      "\n",
      "Joe Biden says Lindsey Graham his longtime friend and Senate colleague will regret pursuing an investigation of tâ€¦\n",
      "Predicted: Neutral\n",
      "\n",
      "Real: Neutral\n",
      "\n",
      "Mall Operator Simon Property Buying a Major Competitor\n",
      "Predicted: Bearish\n",
      "\n",
      "Real: Neutral\n",
      "\n",
      "â€œPeriods aren't just a women's issue it's a human issue\" Meet @nadyaokamoto an activist whose fight for genderâ€¦\n",
      "Predicted: Neutral\n",
      "\n",
      "Real: Neutral\n",
      "\n",
      "Job Growth Surges In January Beating Wall Street Expectations\n",
      "Predicted: Bearish\n",
      "\n",
      "Real: Bullish\n",
      "\n",
      "Nickel Monthly News For The Month Of January 2020  #markets #economy #stocks\n",
      "Predicted: Neutral\n",
      "\n",
      "Real: Neutral\n",
      "\n",
      "Venture capitalists are embracing direct listings and curbing fees for bankers\n",
      "Predicted: Neutral\n",
      "\n",
      "Real: Neutral\n",
      "\n",
      "PhillipsVan Heusen Q3 2020 Earnings Preview\n",
      "Predicted: Neutral\n",
      "\n",
      "Real: Neutral\n",
      "\n",
      "A South African grieving family took a man's corpse to an insurance company to prove he's dead  Here's why theirâ€¦\n",
      "Predicted: Neutral\n",
      "\n",
      "Real: Neutral\n",
      "\n",
      "ðŸ–¥ï¸ Google new quantum computer that can perform computations in 200 seconds that otherwise would take 10000 years\n",
      "Predicted: Bullish\n",
      "\n",
      "Real: Neutral\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent, label in test_data[:10]:\n",
    "    print(\" \".join([id_word[word] for word in sent]))\n",
    "\n",
    "    x = torch.tensor(sent, dtype=torch.long, device=device)\n",
    "    y_pred = rnn(x)\n",
    "\n",
    "    if y_pred.argmax().item() == 0:\n",
    "        print('Predicted: Bearish\\n')\n",
    "        \n",
    "    if y_pred.argmax().item() == 1:\n",
    "        print('Predicted: Bullish\\n')\n",
    "\n",
    "    if y_pred.argmax().item() == 2:\n",
    "        print('Predicted: Neutral\\n')\n",
    "\n",
    "    if label == 0:\n",
    "        print('Real: Bearish\\n')\n",
    "        \n",
    "    if label == 1:\n",
    "        print('Real: Bullish\\n')\n",
    "\n",
    "    if label == 2:\n",
    "        print('Real: Neutral\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
